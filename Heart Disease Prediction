## Load Basic Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
## Reading the csv file
df = pd.read_csv('heart.csv')
df.head()


## Exploring the data set in order to derive useful information
df.shape
this shows our data has 303 rows and 14 columns
df.columns
we can see the column names here 
df.describe()
Statstical Details
Describe provides us with statistical information in  the numerical format.
we can infer that in the AGE column the minimum age is 29yrs and maximium is 77yrs 
mean of age is 54yrs.
The quartiles details are given in form of 25%, 50% and 75%. The data is divided into 3 quartiles
or 4 equal parts. so 25% values lie in each group.
standard deviation and mean are statistical measures which give us an idea of the central tendency 
of the data set. However , mean is effected by outliers and hence we need more information 
to make accurate decisions.


df.isnull().sum()
print(df.info())
we notice that the dataset has no null values.
this saved us from conveting the null values into some data or dropping then altogether.
## Finding the correlation among the attributes
plt.figure(figsize=(20,10))
sns.heatmap(df.corr(), annot=True, cmap='terrain')
we observe positive correlation between target and cp, thalach,slope
and also negative correlation between target and sex, exang,ca,thai,oldpeak
sns.pairplot(data=df)
df.hist(figsize=(12,12), layout=(5,3));
# box and whiskers plot
df.plot(kind='box', subplots=True, layout=(5,3), figsize=(12,12))
plt.show()

sns.catplot(data=df, x='sex', y='age',  hue='target', palette='husl')
sns.barplot(data=df, x='sex', y='chol', hue='target', palette='spring')
df['sex'].value_counts()
#### 207 males and 96 females
df['target'].value_counts()
#### 165 cases of heart diseases
and 138 cases of no heart diseases
df['thal'].value_counts()
#### results of thallium stress test measuring blood flow to the heart,
with possible values normal, fixed_defect, reversible_defect
sns.countplot(x='sex', data=df, palette='husl', hue='target')
Here 1 means male and 0 denotes female. 
we observe female having heart disease are comparatively less when compared to males
Males have low heart diseases as compared to females in the given dataset.
sns.countplot(x='target',palette='BuGn', data=df)
we observe the count for not having heart disease and having heart disease are almost balanced
not having frequency count is 140
and those having heart disease the count is 160.
sns.countplot(x='ca',hue='target',data=df)
##### ca : number of major vessels (0-3) colored by flourosopy
df['ca'].value_counts()
ca number of major vessels (0-3) colored by flourosopy
ca has a negative corelation with the target i.e when ca will increase we witness a drop in 
heart diseases and vice versa.
sns.countplot(x='thal',data=df, hue='target', palette='BuPu' )
thal3 = normal; 6 = fixed defect; 7 = reversable defect
sns.countplot(x='thal', hue='sex',data=df, palette='terrain')
df['cp'].value_counts()  # chest pain type
sns.countplot(x='cp' ,hue='target', data=df, palette='rocket')
shows chest pain with respect to heart disease/ target
sns.countplot(x='cp', hue='sex',data=df, palette='BrBG')
this shows chest pain count experienced by male and female
sns.boxplot(x='sex', y='chol', hue='target', palette='seismic', data=df)
we observe the outliers with the help of boxplot. outliers are values that are very small 
or large in the given data set.
sns.barplot(x='sex', y='cp', hue='target',data=df, palette='cividis')
sns.barplot(x='sex', y='thal', data=df, hue='target', palette='nipy_spectral')
sns.barplot(x='target', y='ca', hue='sex', data=df, palette='mako')
sns.barplot(x='sex', y='oldpeak', hue='target', palette='rainbow', data=df)
##### ST depression induced by exercise relative to rest, a measure of abnormality in electrocardiograms
### fbs(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) and chest pain relation
df['fbs'].value_counts()
sns.barplot(x='fbs', y='chol', hue='target', data=df,palette='plasma' )
sns.barplot(x='sex',y='target', hue='fbs',data=df)
### Cross Tables
gen = pd.crosstab(df['sex'], df['target'])
print(gen)
gen.plot(kind='bar', stacked=True, color=['green','yellow'], grid=False)
temp=pd.crosstab(index=df['sex'],
            columns=[df['thal']], 
            margins=True)
temp
temp.plot(kind="bar",stacked=True)
plt.show()
temp=pd.crosstab(index=df['target'],
            columns=[df['thal']], 
            margins=True)
temp
temp.plot(kind='bar', stacked=True)
plt.show()
chest_pain = pd.crosstab(df['cp'], df['target'])
chest_pain
chest_pain.plot(kind='bar', stacked=True, color=['purple','blue'], grid=False)
## KEEP PRACTICING AND ENJOY THE PROCESS
# Preparing the data for Model 
### Scaling the data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
StandardScaler = StandardScaler()  
columns_to_scale = ['age','trestbps','chol','thalach','oldpeak']
df[columns_to_scale] = StandardScaler.fit_transform(df[columns_to_scale])
df.head()
X= df.drop(['target'], axis=1)
y= df['target']
X_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=40)
### Check the sample Size
print('X_train-', X_train.size)
print('X_test-',X_test.size)
print('y_train-', y_train.size)
print('y_test-', y_test.size)
### Logistic Regression
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()

model1=lr.fit(X_train,y_train)
prediction1=model1.predict(X_test)
from sklearn.metrics import confusion_matrix

cm=confusion_matrix(y_test,prediction1)
cm
sns.heatmap(cm, annot=True,cmap='BuPu')
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('Testing Accuracy:',(TP+TN)/(TP+TN+FN+FP))
from sklearn.metrics import accuracy_score
accuracy_score(y_test,prediction1)
from sklearn.metrics import classification_report
print(classification_report(y_test, prediction1))
## Decision Tree
from sklearn.tree import DecisionTreeClassifier

dtc=DecisionTreeClassifier()
model2=dtc.fit(X_train,y_train)
prediction2=model2.predict(X_test)
cm2= confusion_matrix(y_test,prediction2)
cm2
accuracy_score(y_test,prediction2)

print(classification_report(y_test, prediction2))
## Random Forest
from sklearn.ensemble import RandomForestClassifier

rfc=RandomForestClassifier()
model3 = rfc.fit(X_train, y_train)
prediction3 = model3.predict(X_test)
confusion_matrix(y_test, prediction3)
accuracy_score(y_test, prediction3)

print(classification_report(y_test, prediction3))
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

svm=SVC()
model4=svm.fit(X_train,y_train)
prediction4=model4.predict(X_test)
cm4= confusion_matrix(y_test,prediction4)
cm4
accuracy_score(y_test, prediction4)
from sklearn.naive_bayes import GaussianNB

NB = GaussianNB()
model5 = NB.fit(X_train, y_train)
prediction5 = model5.predict(X_test)
cm5= confusion_matrix(y_test, prediction5)
cm5
accuracy_score(y_test, prediction5)
print('cm4', cm4)
print('-----------')
print('cm5',cm5)

from sklearn.neighbors import KNeighborsClassifier

KNN = KNeighborsClassifier()
model6 = KNN.fit(X_train, y_train)
prediction6 = model6.predict(X_test)
cm6= confusion_matrix(y_test, prediction5)
cm6
print('KNN :', accuracy_score(y_test, prediction6))
print('lr :', accuracy_score(y_test, prediction1))
print('dtc :', accuracy_score(y_test, prediction2))
print('rfc :', accuracy_score(y_test, prediction3))
print('NB: ', accuracy_score(y_test, prediction4))
print('SVC :', accuracy_score(y_test, prediction5))

## Best accuracy is given by Logistic Regression : 92

## followed by NB and Decision tree : 90
